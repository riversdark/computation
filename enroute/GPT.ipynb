{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT in Numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement the GPT2 algorithm in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The quick brown fox jumps over the lazy dog.\"\n",
    "n_tokens_to_generate = 40\n",
    "model_size = \"124M\"\n",
    "models_dir = \"gpt2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from functools import lru_cache\n",
    "\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.errors = errors  # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = \" \".join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \"\".join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(model_name, models_dir):\n",
    "    with open(os.path.join(models_dir, model_name, \"encoder.json\"), \"r\") as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, \"vocab.bpe\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
    "    return Encoder(encoder=encoder, bpe_merges=bpe_merges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, w, b):  # [m, in], [in, out], [out] -> [m, out]\n",
    "    return x @ w + b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, g, b, eps: float = 1e-5):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    x = (x - mean) / np.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis\n",
    "    return g * x + b  # scale and offset with gamma/beta params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn(x, c_fc, c_proj):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # project up\n",
    "    a = gelu(linear(x, **c_fc))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
    "\n",
    "    # project back down\n",
    "    x = linear(a, **c_proj)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask):  # [n_q, d_k], [n_k, d_k], [n_k, d_v], [n_q, n_k] -> [n_q, d_v]\n",
    "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multihead_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mha(x, c_attn, c_proj, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # qkv projection\n",
    "    x = linear(x, **c_attn)  # [n_seq, n_embd] -> [n_seq, 3*n_embd]\n",
    "\n",
    "    # split into qkv\n",
    "    qkv = np.split(x, 3, axis=-1)  # [n_seq, 3*n_embd] -> [3, n_seq, n_embd]\n",
    "\n",
    "    # split into heads\n",
    "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))  # [3, n_seq, n_embd] -> [3, n_head, n_seq, n_embd/n_head]\n",
    "\n",
    "    # causal mask to hide future inputs from being attended to\n",
    "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10  # [n_seq, n_seq]\n",
    "\n",
    "    # perform attention over each head\n",
    "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]  # [3, n_head, n_seq, n_embd/n_head] -> [n_head, n_seq, n_embd/n_head]\n",
    "\n",
    "    # merge heads\n",
    "    x = np.hstack(out_heads)  # [n_head, n_seq, n_embd/n_head] -> [n_seq, n_embd]\n",
    "\n",
    "    # out projection\n",
    "    x = linear(x, **c_proj)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    # multi-head causal self attention\n",
    "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # position-wise feed forward network\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):  # [n_seq] -> [n_seq, n_vocab]\n",
    "    # token + positional embeddings\n",
    "    x = wte[inputs] + wpe[range(len(inputs))]  # [n_seq] -> [n_seq, n_embd]\n",
    "\n",
    "    # forward pass through n_layer transformer blocks\n",
    "    for block in blocks:\n",
    "        x = transformer_block(x, **block, n_head=n_head)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "\n",
    "    # projection to vocab\n",
    "    x = layer_norm(x, **ln_f)  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
    "    return x @ wte.T  # [n_seq, n_embd] -> [n_seq, n_vocab]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    for _ in tqdm(range(n_tokens_to_generate), \"generating\"):  # auto-regressive decode loop\n",
    "        logits = gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
    "        next_id = np.argmax(logits[-1])  # greedy sampling\n",
    "        inputs.append(int(next_id))  # append prediction to input\n",
    "\n",
    "    return inputs[len(inputs) - n_tokens_to_generate :]  # only return generated ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gpt2_files(model_size, model_dir):\n",
    "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
    "    for filename in [\n",
    "        \"checkpoint\",\n",
    "        \"encoder.json\",\n",
    "        \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\",\n",
    "        \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\",\n",
    "        \"vocab.bpe\",\n",
    "    ]:\n",
    "        url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "        r = requests.get(f\"{url}/{model_size}/{filename}\", stream=True)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        with open(os.path.join(model_dir, filename), \"wb\") as f:\n",
    "            file_size = int(r.headers[\"content-length\"])\n",
    "            chunk_size = 1000\n",
    "            with tqdm(\n",
    "                ncols=100,\n",
    "                desc=\"Fetching \" + filename,\n",
    "                total=file_size,\n",
    "                unit_scale=True,\n",
    "                unit=\"b\",\n",
    "            ) as pbar:\n",
    "                # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
    "                for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams):\n",
    "    def set_in_nested_dict(d, keys, val):\n",
    "        if not keys:\n",
    "            return val\n",
    "        if keys[0] not in d:\n",
    "            d[keys[0]] = {}\n",
    "        d[keys[0]] = set_in_nested_dict(d[keys[0]], keys[1:], val)\n",
    "        return d\n",
    "\n",
    "    params = {\"blocks\": [{} for _ in range(hparams[\"n_layer\"])]}\n",
    "    for name, _ in tf.train.list_variables(tf_ckpt_path):\n",
    "        array = np.squeeze(tf.train.load_variable(tf_ckpt_path, name))\n",
    "        name = name[len(\"model/\") :]\n",
    "        if name.startswith(\"h\"):\n",
    "            m = re.match(r\"h([0-9]+)/(.*)\", name)\n",
    "            n = int(m[1])\n",
    "            sub_name = m[2]\n",
    "            set_in_nested_dict(params[\"blocks\"][n], sub_name.split(\"/\"), array)\n",
    "        else:\n",
    "            set_in_nested_dict(params, name.split(\"/\"), array)\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoder_hparams_and_params(model_size, models_dir):\n",
    "    assert model_size in [\"124M\", \"355M\", \"774M\", \"1558M\"]\n",
    "\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    if not tf_ckpt_path:  # download files if necessary\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        download_gpt2_files(model_size, model_dir)\n",
    "        tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "\n",
    "    encoder = get_encoder(model_size, models_dir)\n",
    "    hparams = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, hparams)\n",
    "\n",
    "    return encoder, hparams, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load encoder, hparams, and params from the released open-ai gpt-2 files\n",
    "encoder, hparams, params = load_encoder_hparams_and_params(model_size, models_dir)\n",
    "\n",
    "# encode the input string using the BPE tokenizer\n",
    "input_ids = encoder.encode(prompt)\n",
    "\n",
    "# make sure we are not surpassing the max sequence length of our model\n",
    "assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n",
    "\n",
    "# generate output ids\n",
    "output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n",
    "\n",
    "# decode the ids back into a string\n",
    "output_text = encoder.decode(output_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
