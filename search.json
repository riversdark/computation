[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assorted essays",
    "section": "",
    "text": "Click on the links on the left to navigate to the different sections.\n\n01 Omics Data Analysis\nGenomics, transcriptomics, proteomics, metabolomics, lipidomics, etc., uncovering the molecular mechanisms by studying their sequence information.\n\n\n02 Computational protein design\nDesigning functional proteins, peptides, enzymes, antibodies, nanobodies, etc.\n\n\n03 Computational micromolecule design\nDesigning small molecules that bind to protein targets, this includes inhibitors, activators, allosteric modulators, etc.\n\n\n04 Molecular dynamics\nComputer simulations to understand the molecular mechanisms of biological systems.\n\n\n05 Machine learning\nIn depth analysis of key machine learning models and their applications.\n\n\n05 New technologies\nNew technologies under investigation."
  },
  {
    "objectID": "01_0.html",
    "href": "01_0.html",
    "title": "Omics Data Analysis",
    "section": "",
    "text": "what’s the difference between genomics and transcriptomics? Genomics is the study of an organism’s entire genome, which includes all of its genes and their interactions. Transcriptomics is the study of all of the transcripts, or RNA molecules, that are produced from an organism’s genes. Transcriptomics provides a more detailed look at the expression of genes and how they are regulated, while genomics looks at the entire genome and how it is organized and functions.\nwhat’s the difference between proteomics and transcriptomics? Proteomics is the large-scale study of proteins, including their structure, function, and interactions, while transcriptomics is the study of transcriptomes, which are the complete set of RNA molecules, including mRNA, tRNA, and rRNA, produced by the transcription of DNA. Proteomics focuses on the proteins that are expressed in cells, while transcriptomics focuses on the transcripts that are produced and the role they play in gene expression."
  },
  {
    "objectID": "01_0.html#genomics",
    "href": "01_0.html#genomics",
    "title": "Omics Data Analysis",
    "section": "Genomics",
    "text": "Genomics\nGenomics is the study of an organism’s entire genome, which includes all of its genes and their interactions. Genomics is an interdisciplinary field that combines the techniques of genetics, molecular biology, bioinformatics, and computer science to study the structure, function, evolution, and regulation of genomes. Genomics is used to study the genetic makeup of organisms, including humans, plants, and animals, and to understand how genes are inherited and how they interact with each other and with the environment."
  },
  {
    "objectID": "01_0.html#transcriptomics",
    "href": "01_0.html#transcriptomics",
    "title": "Omics Data Analysis",
    "section": "Transcriptomics",
    "text": "Transcriptomics"
  },
  {
    "objectID": "01_0.html#proteomics",
    "href": "01_0.html#proteomics",
    "title": "Omics Data Analysis",
    "section": "Proteomics",
    "text": "Proteomics"
  },
  {
    "objectID": "01_0.html#metagenomics",
    "href": "01_0.html#metagenomics",
    "title": "Omics Data Analysis",
    "section": "Metagenomics",
    "text": "Metagenomics\nMetagenomics is a field of study that focuses on the genomic analysis of entire microbial communities, typically including bacteria, archaea, and viruses, in an environmental sample. It is an interdisciplinary field that combines the techniques of genomics, bioinformatics, and ecology to study the collective genomes of microbial communities. Metagenomics can provide insight into the functions and interactions of microbial communities and their roles in various environments.\nwhat are the procedures of metagenomics?\n\nSample Collection: Metagenomics begins with the collection of an environmental sample, such as soil, water, or air.\nDNA Extraction: The DNA is then extracted from the sample, typically using a method such as lysis or centrifugation.\nLibrary Preparation: The extracted DNA is then prepared for sequencing in a process known as library preparation.\nSequencing: The prepared DNA is then sequenced using next-generation sequencing (NGS) or other methods.\nData Analysis: The resulting sequence data is then analyzed using bioinformatics tools and methods.\nResults Interpretation: The results of the analysis are then interpreted and used to draw conclusions about the microbial community and its role in the environment.\n\nwhat are the main developments in metagenomics in the last ten years?\n\nAdvances in sequencing technologies: Over the past decade, sequencing technologies have advanced significantly, creating a more cost-effective and efficient way to collect data on microbial communities. This has enabled researchers to generate unprecedented amounts of data, leading to new insights into the microbial world.\nBioinformatics advances: With the increase in sequencing data, the development of powerful bioinformatics tools has become critical to extracting meaningful information from metagenomic data. As a result, new analysis pipelines have been developed to compare metagenomes, detect patterns in metagenomic data, and identify novel microbial species.\nIncreased understanding of human microbiome: The development of metagenomics has enabled researchers to gain a better understanding of the human microbiome and its role in health and disease. This research has provided insights into how the microbiome contributes to various diseases and how it can be modified to improve human health.\nEnvironmental metagenomics: Metagenomics has also been used to explore microbial communities in the environment. This research has provided insights into how microbial communities interact with their environment, how they respond to environmental changes, and how they can be used to monitor environmental health."
  },
  {
    "objectID": "01_0.html#metabolomics",
    "href": "01_0.html#metabolomics",
    "title": "Omics Data Analysis",
    "section": "metabolomics",
    "text": "metabolomics\nMetabolomics is the study of the unique chemical fingerprints that specific cellular processes leave behind. It is an emerging field of study focused on the comprehensive analysis of the small molecule metabolites present in a biological sample, such as those from cells, tissues, or fluids. Through metabolomics, researchers can better understand the biochemical pathways and processes that are occurring in the body, providing insight into physiological states, disease processes, and responses to treatments."
  },
  {
    "objectID": "02_0.html",
    "href": "02_0.html",
    "title": "Computational protein design",
    "section": "",
    "text": "Designing functional proteins, peptides, enzymes, antibodies, nanobodies, etc."
  },
  {
    "objectID": "03_0.html",
    "href": "03_0.html",
    "title": "Computational micromolecule design",
    "section": "",
    "text": "Inhibitors are small molecules that bind to a protein target and prevent it from functioning. They are used to treat a variety of diseases, including cancer, diabetes, and cardiovascular disease. Inhibitors are typically designed to bind to a specific protein target, such as an enzyme, and prevent it from functioning. They are often used to treat diseases that are caused by an overactive protein, such as cancer, diabetes, and cardiovascular disease.\nActivators are small molecules that bind to a protein target and increase its activity. Allosteric modulators are small molecules that bind to a protein target and change its activity."
  },
  {
    "objectID": "06_0.html",
    "href": "06_0.html",
    "title": "New technologies",
    "section": "",
    "text": "氘代技术"
  },
  {
    "objectID": "00_openai.html",
    "href": "00_openai.html",
    "title": "Scientific Computation at large",
    "section": "",
    "text": "# Set your secret API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")"
  },
  {
    "objectID": "01_pyro_cov.html",
    "href": "01_pyro_cov.html",
    "title": "pyro_cov",
    "section": "",
    "text": "RSV and SARS-CoV-2 using Pyro\nnextstrain offers a nice way to download data from GISAID.\nusher is a tool to build a tree from a set of sequences.\ncov-lineages/pango-designation suggests new lineages that should be added to the current scheme.\ncov-lineages/pangoLEARN is a Store of the trained model for pangolin to access. This repository is deprecated and only for use with pangolin 2.0 and 3.0. For latest pangolin data models compatible with pangolin 4.0, use cov-lineages/pangolin-data, the repo for storing latest model, protobuf, designation hash and alias files for pangolin assignments\nCSSEGISandData/COVID-19 is a repository with Novel Coronavirus (COVID-19) Cases, provided by JHU CSSE.\nnextstrain/nextclade is a tool for Viral genome alignment, mutation calling, clade assignment, quality checks and phylogenetic placement\nManhattan plot of the mutation rates for the SARS-CoV-2 genome. The plot shows the log10 of the p-values for the null hypothesis that the mutation rate is zero. The plot is based on the results of the analysis of 6.4 million SARS-CoV-2 genomes. The plot shows that the mutation rate is significantly different from zero for 11 of the 29 genes in the SARS-CoV-2 genome. The genes with the highest mutation rates are ORF1ab, N, and S. The genes with the lowest mutation rates are E, M, and NSP3. The plot also shows that the mutation rate is significantly different from zero for the entire genome. The plot is based on the results of the analysis of 6.4 million SARS-CoV-2 genomes. The plot shows that the mutation rate is significantly different from zero for 11 of the 29 genes in the SARS-CoV-2 genome. The genes with the highest mutation rates are ORF1ab, N, and S. The genes with the lowest mutation rates are E, M, and NSP3. The plot also shows that the mutation rate is significantly different from zero for the entire genome.\n\nimport os\nimport sys\n\nREPO_ADDRESS = \"https://github.com/broadinstitute/pyro-cov.git\"\nREPO_NAME = \"pyro-cov\"\n\n# Download the repo if it doesn't exist\nif not os.path.exists(REPO_NAME):\n    !git clone $REPO_ADDRESS\n\n# change to the repo directory\nos.chdir(REPO_NAME)\n\n\n!pip install -e .\n\nDownload data\n\n# download the data\n!make update\n\nPreprocess data\nThis takes under an hour.\n\n!make preprocess\n\nanalyze data\n\n# analyze data\n!python scripts/mutrans.py --vary-gene"
  },
  {
    "objectID": "01_wgs.html",
    "href": "01_wgs.html",
    "title": "Scientific Computation at large",
    "section": "",
    "text": "clarify the choice of reference genome\nclarify the choice of known sites\nmodify the code to suit for multiple samples\n\n\n\nTools necessary for WGS variant calling using GATK include:\nPicard is a Java-based toolkit for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF.\nGATK is a toolkit for variant discovery in high-throughput sequencing data. It is designed to work best with data generated by next-generation DNA sequencing technologies, but can be applied to other types of data as well.\nSamtools is a suite of programs for interacting with high-throughput sequencing data. It provides various utilities for post-processing alignments in the SAM, BAM and CRAM formats, such as indexing, variant calling, viewing alignments, and generating alignments in a per-position format. It also includes a large number of utilities for manipulating alignments in a variety of different ways, for example, to extract all reads that map to a particular region of the genome.\nBWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the rest two for longer sequences ranged from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate. BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads. For all the algorithms, BWA first needs to construct the FM-index for the reference genome (the index command). Alignment algorithms are invoked with different sub-commands: aln/samse/sampe for BWA-backtrack, bwasw for BWA-SW and mem for the BWA-MEM algorithm.\nFastQC is a quality control tool for high throughput sequence data. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis. It produces a simple HTML file with a summary of the results of the analysis, which can be viewed in a web browser.\nOther tools that are useful for WGS variant calling include: - Freebayes, a Bayesian genetic variant detector designed to find small polymorphisms, specifically SNPs (single-nucleotide polymorphisms), indels (insertions and deletions), MNPs (multi-nucleotide polymorphisms), and complex events (composite insertion and substitution events) - bcftools, a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF - vcftools, a set of tools written in Perl and C++ for working with VCF files, such as those generated by the 1000 Genomes Project - bedtools, a suite of utilities for genome arithmetic - tabix, a generic indexer for TAB-delimited genome position files - bgzip, a small utility that enables tabix to work with gzip-compressed files\n\n\n\ndefine the reference genome directory and files\n\nimport os\n\nREF_DIR = \"/mnt/nas/wgs/hg38\"\nREF_URL = \"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\"\nREF_FILE = os.path.join(REF_DIR, \"hg38.fa\")\nKNOWN_SITES = os.path.join(REF_DIR, \"Homo_sapiens_assembly38.dbsnp138.vcf\")\nKNOWN_SITES_IDX = os.path.join(REF_DIR, \"Homos_sapiens_assembly38.dbsnp138.vcf.idx\")\n\ndownload reference genome and index it\n\n# Download the reference genome if the file doesn't exist\nif not os.path.exists(os.path.join(REF_DIR, \"hg38.fa\")):\n    !wget -O $REF_DIR/hg38.fa.gz $REF_URL\n    !gunzip $REF_DIR/hg38.fa.gz\n\n# index the reference genome file before running haplotype caller\nif not os.path.exists(os.path.join(REF_DIR, \"hg38.fa.fai\")):\n    !samtools faidx $REF_DIR/hg38.fa\n\n# Create dictionary file for the reference genome if it doesn't exist\nif not os.path.exists(os.path.join(REF_DIR, \"hg38.dict\")):\n    !gatk CreateSequenceDictionary R=$REF_DIR/hg38.fa O=$REF_DIR/hg38.dict\n\n# index the reference genome file using BWA, if the index files don't exist\nif not os.path.exists(os.path.join(REF_DIR, \"hg38.fa.sa\")):\n    !bwa index $REF_FILE\n\nSITE_URL = \"https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf\"\nSITE_IDX_URL = \"https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf.idx\"\n\n# Download known sites files for BQSR (Base Quality Score Recalibration) from GATK resource bundle if they don't exist\nif not os.path.exists(os.path.join(REF_DIR, \"Homo_sapiens_assembly38.dbsnp138.vcf\")):\n    !wget -O $REF_DIR $SITE_URL\n\nif not os.path.exists(os.path.join(REF_DIR, \"Homo_sapiens_assembly38.dbsnp138.vcf.idx\")):\n    !wget -O $REF_DIR $SITE_IDX_URL\n\n# show the contents of the reference directory\n!ls -halt $REF_DIR\n\n\n\n\n\nREADS_DIR = \"/mnt/nas/wgs/RDD2206P0001\"\nREADS_FILE1 = os.path.join(READS_DIR, \"RDD2206P0001-WGS-1_R1.fastq.gz\")\nREADS_FILE2 = os.path.join(READS_DIR, \"RDD2206P0001-WGS-1_R2.fastq.gz\")\n\n\n\n\nThis is a quick overview of the quality of the data. We will use FastQC to generate a report for each sample.\n\n# Run FastQC on the reads\n!fastqc $READS_FILE1 $READS_FILE2 -o $READS_DIR\n\n\n\n\nthe aligner bwa aln can trim the reads to remove low quality bases at the end of the reads. This is done using the -q option. The default value is 15.\nThe BWA-MEM algorithm performs local alignment. It may produce multiple primary alignments for different part of a query sequence. This is a crucial feature for long sequences. However, some tools such as Picard’s markDuplicates does not work with split alignments. One may consider to use option -M to flag shorter split hits as secondary. This option is not recommended for Illumina short reads.\n\n# Align the reads to the reference genome\n!bwa mem -t 50 -M -R \"@RG\\tID:RDD2206P0001\\tSM:RDD2206P0001\" $REF_FILE $READS_FILE1 $READS_FILE2 > $READS_DIR/RDD2206P0001-WGS-1.sam\n\n-R marks the complete read group header line. ’ can be used in STR and will be converted to a TAB in the output SAM. The read group ID will be attached to every read in the output. The read group ID is used to identify the read group in the output file. The read group sample name is used to identify the sample in the output file.\n\n\nthe SAM file is a text file that contains the alignment of the reads to the reference genome. SAM stands for Sequence Alignment/Map format.\n\n# Show the first 10 lines of the SAM file\n!head -n 10 $READS_DIR/RDD2206P0001-WGS-1.sam\n\nWe can view the flag statistics of the SAM file using samtools flagstat command. The output of this command is a table that shows the number of reads that have a particular flag.\n\n# Show the flag statistics\n!samtools flagstat $READS_DIR/RDD2206P0001-WGS-1.sam\n\nThe SAM file contains the following flags:\n\n0x1: template having multiple segments in sequencing\n0x2: each segment properly aligned according to the aligner\n0x4: segment unmapped\n0x8: next segment in the template unmapped\n0x10: SEQ being reverse complemented\n0x20: SEQ of the next segment in the template being reversed\n0x40: the first segment in the template\n0x80: the last segment in the template\n0x100: secondary alignment\n0x200: not passing quality controls\n0x400: PCR or optical duplicate\n0x800: supplementary alignment\n\nThe SAM file contains the following fields:\n\nQNAME: Query template NAME\nFLAG: bitwise FLAG\nRNAME: Reference sequence NAME\nPOS: 1-based leftmost mapping POSition\nMAPQ: MAPping Quality\nCIGAR: CIGAR string\nRNEXT: Ref. name of the mate/next read\nPNEXT: Position of the mate/next read\nTLEN: observed Template LENgth\nSEQ: segment SEQuence\nQUAL: ASCII of Phred-scaled base QUALity+33\n\nThe SAM file needs to be sorted by the read name. This is done to make the file compatible with MarkDuplicatesSpark and HaplotypeCallerSpark.\n\n\n\n\nthe BAM file is a binary file that contains the alignment of the reads to the reference genome. BAM stands for Binary Alignment/Map format. BAM is a compressed binary version of SAM.\n\n!samtools view -bS $READS_DIR/RDD2206P0001-WGS-1.sam > $READS_DIR/RDD2206P0001-WGS-1.bam\n\n\n\n\nThe BAM file is sorted by the read name. This is done to make the file compatible with MarkDuplicatesSpark and HaplotypeCallerSpark.\n\n!samtools sort -@ 50 $READS_DIR/RDD2206P0001-WGS-1.bam -o $READS_DIR/RDD2206P0001-WGS-1.sorted.bam\n\n\n# Show the flag statistics\n!samtools flagstat $READS_DIR/RDD2206P0001-WGS-1.sorted.bam\n\n\n\n\nMark duplicates in the BAM file. This is done to remove PCR duplicates. PCR duplicates are reads that are generated from the same DNA fragment. PCR duplicates are generated during the PCR amplification step. PCR duplicates are removed to avoid overestimation of the coverage of the genome.\nMarkDuplicatesSpark is a Spark version of MarkDuplicates. It is faster than MarkDuplicates and can be run on a cluster.\n\n!gatk MarkDuplicatesSpark -I $READS_DIR/RDD2206P0001-WGS-1.sorted.bam -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam -M $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.metrics\n\n\n# Show the flag statistics\n!samtools flagstat $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam\n\n\n\n\nIndex the BAM file. This is done to make the file compatible with HaplotypeCallerSpark.\n\n!samtools index $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam\n\n\n\n\nBase quality score recalibration (BQSR) is a process that recalibrates the base quality scores of the reads. BQSR is done to correct for systematic errors in the base quality scores. BQSR is done using the BaseRecalibrator and ApplyBQSR tools. The BaseRecalibrator tool generates a recalibration table. The ApplyBQSR tool applies the recalibration table to the BAM file.\n\n!gatk BaseRecalibrator -I $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam -R $REF_FILE -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal_data.table --known-sites $KNOWN_SITES\n!gatk ApplyBQSR -R $REF_FILE -I $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam --bqsr-recal-file $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal_data.table -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.bam\n\n\n\n\nThe HaplotypeCaller tool is used to call variants. The HaplotypeCaller tool is a Spark version of the HaplotypeCaller tool. It is faster than the HaplotypeCaller tool and can be run on a cluster.\n\n!gatk HaplotypeCaller -R $REF_FILE -I $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.bam -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.vcf\n\n\n\n\n\n!gatk VariantFiltration -R $REF_FILE -V $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.vcf -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.vcf --filter-expression \"QD < 2.0\" --filter-name \"QD2\" --filter-expression \"FS > 60.0\" --filter-name \"FS60\" --filter-expression \"MQ < 40\n\n\n\n\n\n!gatk VariantAnnotator -R $REF_FILE -V $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.vcf -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.vcf --dbsnp $KNOWN_SITES\n\n\n\n\n\n!gatk VariantRecalibrator -R $REF_FILE -V $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.vcf -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.vcf --tranches-file $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.tranches --rscript-file $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.plots.R --resource:hapmap,known=false,training=true,truth=true,prior=15.0 $HAPMAP --resource:omni,known=false,training=true,truth=false,prior=12.0 $OMNI --resource:1000G,known=false,training=true,truth=false,prior=10.0 $KG --resource:dbsnp,known=true,training=false,truth=false,prior=2.0 $KNOWN_SITES --an QD --an MQ --an MQRankSum --an ReadPosRankSum --an FS --an SOR --mode SNP --tranche 100.0 --tranche 99.9 --tranche 99.0 --tranche 90.0 --max-gaussians 4\n\n\n\n\n\n!gatk ApplyVQSR -R $REF_FILE -V $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.vcf -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.vcf --ts-filter-level 99.0 --tranches-file $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.tranches --recal-file $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.vcf --mode SNP\n\n\n\n\nREF_DIR=\"/mnt/nas/wgs/hg38\"\nREF_URL=\"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\"\nREF_FILE=\"/mnt/nas/wgs/hg38/hg38.fa\"\nKNOWN_SITES_FILE=\"/mnt/nas/wgs/hg38/Homo_sapiens_assembly38.dbsnp138.vcf\"\nKNOWN_SITES_IDX_FILE=\"/mnt/nas/wgs/hg38/Homo_sapiens_assembly38.dbsnp138.vcf.idx\"\nREADS_DIR=\"/mnt/nas/wgs/RDD2206P0001\"\nREADS_FILE1=\"/mnt/nas/wgs/RDD2206P0001/RDD2206P0001-WGS-1_R1.fastq.gz\"\nREADS_FILE2=\"/mnt/nas/wgs/RDD2206P0001/RDD2206P0001-WGS-1_R2.fastq.gz\"\n\n# Run FastQC on the reads\nfastqc $READS_FILE1 $READS_FILE2 -o $READS_DIR\n\n# Align the reads to the reference genome\nbwa mem -t 50 -M -R \"@RG\\tID:RDD2206P0001\\tSM:RDD2206P0001\" $REF_FILE $READS_FILE1 $READS_FILE2 > $READS_DIR/RDD2206P0001-WGS-1.sam\n\n# Convert the SAM file to BAM and sort it\nsamtools view -bS $READS_DIR/RDD2206P0001-WGS-1.sam > $READS_DIR/RDD2206P0001-WGS-1.bam\nsamtools sort -@ 50 $READS_DIR/RDD2206P0001-WGS-1.bam -o $READS_DIR/RDD2206P0001-WGS-1.sorted.bam\n\n# Mark duplicates\ngatk MarkDuplicatesSpark -I $READS_DIR/RDD2206P0001-WGS-1.sorted.bam -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam --METRICS_FILE $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.metrics\n\n# Show the flag statistics\nsamtools flagstat $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam > $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.flagstat\n\n# Index the BAM file\nsamtools index $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam\n\n# Create a BAI file\nsamtools index $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam\n\n# Base quality score recalibration\ngatk BaseRecalibratorSpark -I $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.grp -R $REF_FILE --known-sites $KNOWN_SITES_FILE\n\n# Apply base quality score recalibration\ngatk ApplyBQSRSpark -I $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.bam --bqsr-recal-file $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.grp -R $REF_FILE\n\n# Collect alignment and insert size metrics\ngatk CollectAlignmentSummaryMetrics -I $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.bam -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.alignment_metrics -R $REF_FILE\ngatk CollectInsertSizeMetrics -I $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.bam -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.insert_size_metrics -H $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.insert_size_histogram.pdf -M 0.5\n\n# Call variants\ngatk HaplotypeCallerSpark -I $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.bam -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.vcf -R $REF_FILE --emit-ref-confidence GVCF --variant-index-type LINEAR --variant-index-parameter 128000\n\n# extract the SNP variants\ngatk SelectVariants -V $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.vcf -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.snp.vcf --select-type-to-include SNP\n\n# extract the INDEL variants\ngatk SelectVariants -V $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.vcf -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.indel.vcf --select-type-to-include INDEL"
  },
  {
    "objectID": "02_AFdesign.html",
    "href": "02_AFdesign.html",
    "title": "Scientific Computation at large",
    "section": "",
    "text": "import os\nimport sys\n\nREPO_ADDRESS = \"https://github.com/sokrypton/ColabDesign.git\"\nREPO_DIR = \"/home/ma/git/computation/ColabDesign\"\n\nif not os.path.exists(REPO_DIR):\n    !git clone {REPO_ADDRESS}\n\n%cd {REPO_DIR}\n\n\nPARAMS_DIR = \"/mnt/nas/alphafold/alphafold_params_2022-12-06/\"\nPARAMS_LOCAL_DIR = \"params\"\n\n# Create a symbolic link to the params directory\nif not os.path.exists(PARAMS_LOCAL_DIR):\n    !ln -s {PARAMS_DIR} {PARAMS_LOCAL_DIR}\n\n\nimport os\nfrom colabdesign import mk_afdesign_model, clear_mem\nfrom IPython.display import HTML\nimport numpy as np\n\n\n# Download PDB file from RCSB, given a PDB ID\ndef download_pdb(pdb_id):\n    pdb_id = pdb_id.lower()\n    pdb_filename = f\"{pdb_id}.pdb\"\n    if not os.path.exists(pdb_filename):\n        !wget https://files.rcsb.org/download/{pdb_filename}\n    return pdb_filename\n\n\ndownload_pdb(\"6LU7\")\n\nThe losses being optimised in the design process are defined in the model.set_opt function. The following losses are available:\n\ngeneral losses\n\npae - minimizes the predicted alignment error\nplddt - maximizes the predicted LDDT (local distance difference test)\npae and plddt values are between 0 and 1 (where lower is better for both)\n\nfixbb specific losses\n\ndgram_cce - minimizes the categorical-crossentropy between predicted distogram and one extracted from pdb.\nfape - minimize difference between coordinates (frame aligned point error)\nwe find dgram_cce loss to be more stable for design (compared to fape)\n\nhallucination specific losses\n\ncon - maximize 1 contacts per position. model.set_opt(\"con\",num=1)\n\nbinder specific losses\n\npae - minimize PAE at interface and within binder\ncon - - maximize 2 contacts per binder position, within binder. model.set_opt(\"con\",num=2)\ni_con - maximize 1 contacts per binder position model.set_opt(\"i_con\",num=1)\n\npartial hallucination specific losses\n\nsc_fape - sidechain-specific fape\n\n\nThe pAE (predicted alignment error) measures\ndistogram is defined as\nIn AlphaFold, the plddt score measures the percentage of aligned residues that are within 8 angstroms of the correct position. The rmsd score measures the root-mean-square deviation of the aligned residues from the correct position. The confidence score is the probability that the model is correct. The confidence score is not used in the AlphaFold ranking, but is included for completeness."
  },
  {
    "objectID": "02_AFdesign.html#ten",
    "href": "02_AFdesign.html#ten",
    "title": "Scientific Computation at large",
    "section": "1TEN",
    "text": "1TEN\nFor a given protein backbone, generate/design a new sequence that AlphaFold thinks folds into that conformation.\n\nclear_mem()\naf_model = mk_afdesign_model(protocol=\"fixbb\")\naf_model.prep_inputs(pdb_filename=download_pdb(\"1TEN\"), chain=\"A\")\n\nprint(\"length\",  af_model._len)\nprint(\"weights\", af_model.opt[\"weights\"])\n\n\naf_model.restart()\naf_model.design_3stage()\n\n\naf_model.plot_traj()  \n\nThe plot_traj function plots the training trajectories.\n\naf_model.save_pdb(f\"tenascin_{af_model.protocol}.pdb\")\n\n\nHTML(af_model.animate())\n\n\naf_model.get_seqs()"
  },
  {
    "objectID": "02_AlphaFold.html",
    "href": "02_AlphaFold.html",
    "title": "Scientific Computation at large",
    "section": "",
    "text": "Download the data\nuse the download script provided by DeepMind\n\n\nDownload the model parameters\nDeepmind updates the model parameters every 6 months. The latest model parameters are available at the following link:\nhttps://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar\n\nPARAMS_ADDRESS = \"https://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar\"\nPARAMS_NAME = \"alphafold_params_2022-12-06.tar\"\nPARAMS_DIR = \"/mnt/nas/alphafold/alphafold_params_2022-12-06\"\n\n\n# download params if not already downloaded, create directory if not already created, including parent directories\nif not os.path.exists(PARAMS_DIR):\n    !mkdir -p {PARAMS_DIR}\n    !wget {PARAMS_ADDRESS} -P {PARAMS_DIR}\n    !tar -xvf {PARAMS_DIR}/{PARAMS_NAME} -C {PARAMS_DIR}\n\n\n--2023-01-11 13:53:06--  https://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar\nResolving storage.googleapis.com (storage.googleapis.com)... 172.217.160.112, 142.251.43.16, 172.217.163.48, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|172.217.160.112|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5587968000 (5.2G) [application/x-tar]\nSaving to: ‘/mnt/nas/alphafold/alphafold_params_2022-12-06/alphafold_params_2022-12-06.tar’\n\nalphafold_params_20 100%[===================>]   5.20G  51.9MB/s    in 1m 55s  \n\n2023-01-11 13:55:03 (46.2 MB/s) - ‘/mnt/nas/alphafold/alphafold_params_2022-12-06/alphafold_params_2022-12-06.tar’ saved [5587968000/5587968000]\n\nparams_model_1.npz\nparams_model_2.npz\nparams_model_3.npz\nparams_model_4.npz\nparams_model_5.npz\nparams_model_1_ptm.npz\nparams_model_2_ptm.npz\nparams_model_3_ptm.npz\nparams_model_4_ptm.npz\nparams_model_5_ptm.npz\nparams_model_1_multimer_v3.npz\nparams_model_2_multimer_v3.npz\nparams_model_3_multimer_v3.npz\nparams_model_4_multimer_v3.npz\nparams_model_5_multimer_v3.npz\nLICENSE"
  },
  {
    "objectID": "02_binding_ddg.html",
    "href": "02_binding_ddg.html",
    "title": "Predicting ddG",
    "section": "",
    "text": "# suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nMUT_PROTEIN = \"data/example_mut.pdb\"\nWT_PROTEIN = \"data/example_wt.pdb\"\n\n\n# show the protein chains and number of amino acids\ndef show_protein(protein):\n    from Bio.PDB import PDBParser\n\n    parser = PDBParser()\n    structure = parser.get_structure(\"protein\", protein)\n    for model in structure:\n        for chain in model:\n            print(chain.id, len(chain))\n\nshow_protein(MUT_PROTEIN)\nshow_protein(WT_PROTEIN)\n\nA 214\nB 214\nC 200\nA 214\nB 214\nC 200\n\n\n\n\n# Get Amino Acid sequence from PDB file\ndef get_sequence(pdb_file, chain=\"A\"):\n    from Bio.PDB import PDBParser, PPBuilder\n\n    parser = PDBParser()\n    structure = parser.get_structure(\"protein\", pdb_file)\n    ppb = PPBuilder()\n    for pp in ppb.build_peptides(structure[0][chain]):\n        return pp.get_sequence()\n\n# Get sequence of mutated protein\nmut_seq = get_sequence(MUT_PROTEIN)\n# Get sequence of wild type protein\nwt_seq = get_sequence(WT_PROTEIN)\n\n# Show sequences\nprint(\"Mutated protein sequence: \", mut_seq)\nprint(\"Wild type protein sequence: \", wt_seq)\n\nMutated protein sequence:  DIKMTQSPSSMYASLGERVTITCKASQDIRKYLNWYQQKPWKSPKTLIYYATSLADGVPSRFSGSGSGQDYSLTISSLESDDTATYYCLQHGESPYTFGGGTKLEINRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC\nWild type protein sequence:  DIKMTQSPSSMYASLGERVTITCKASQDIRKYLNWYQQKPWKSPKTLIYYATSLADGVPSRFSGSGSGQDYSLTISSLESDDTATYYCLQHGESPYTFGGGTKLEINRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC\n\n\n\n# Show difference between sequences\ndef show_aa_diff(seq1, seq2):\n    for i in range(len(seq1)):\n        if seq1[i] != seq2[i]:\n            print(f\"Position {i+1}: {seq1[i]} -> {seq2[i]}\")\n\n\nseq1 = \"ABCD\"\nseq2 = \"ABEF\"\nshow_aa_diff(seq1, seq2)\n\nPosition 3: C -> E\nPosition 4: D -> F\n\n\n\n\n# loop through the peptide chains to find the mutations\nfor chain in \"ABC\":\n    print(f\"Chain {chain}:\")\n    show_aa_diff(get_sequence(MUT_PROTEIN, chain), get_sequence(WT_PROTEIN, chain))\n\nChain A:\nChain B:\nChain C:\n\n\n\n# compute the ddG\n!python scripts/predict.py $WT_PROTEIN $MUT_PROTEIN\n\nPredicted ddG: -0.30"
  },
  {
    "objectID": "02_chroma.html",
    "href": "02_chroma.html",
    "title": "Scientific Computation at large",
    "section": "",
    "text": "https://www.biorxiv.org/content/10.1101/2022.12.01.518682v1\nhttps://generatebiomedicines.com/chroma\nhttps://github.com/lucidrains/chroma-pytorch"
  },
  {
    "objectID": "02_RFdiffusion.html",
    "href": "02_RFdiffusion.html",
    "title": "Scientific Computation at large",
    "section": "",
    "text": "Protein monomer design\n\n\nProtein binder design\n\n\nSymmetric oligomer design\nsymmetric oligomers can be used to\n\n\nEnzyme active site scaffolding\nenzyme active sites are\n\n\nSymmetric motif scaffolding (e.g. helix, beta sheet) design\nSymmetric motif scaffolds are designed by specifying the sequence of the motif and the number of repeats. The motif is then repeated in the sequence of the scaffold. The motif is specified by a list of amino acids, where each amino acid is specified by a one-letter code. The number of repeats is specified by an integer."
  },
  {
    "objectID": "03_difflinker.html",
    "href": "03_difflinker.html",
    "title": "Scientific Computation at large",
    "section": "",
    "text": "import os\nimport sys\n\nREPO_ADDRESS = \"https://github.com/igashov/DiffLinker.git\"\nREPO_NAME = \"DiffLinker\"\n\nif not os.path.exists(REPO_NAME):\n    !git clone {REPO_ADDRESS}\n\n%cd {REPO_NAME}\n\nCloning into 'DiffLinker'...\nremote: Enumerating objects: 185, done.\nremote: Counting objects: 100% (185/185), done.\nremote: Compressing objects: 100% (136/136), done.\nremote: Total 185 (delta 81), reused 139 (delta 44), pack-reused 0\nReceiving objects: 100% (185/185), 17.92 MiB | 13.56 MiB/s, done.\nResolving deltas: 100% (81/81), done.\n/home/ma/git/computation/DiffLinker"
  }
]