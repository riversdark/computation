[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assorted essays",
    "section": "",
    "text": "Click on the links on the left to navigate to the different sections.\n\n01 Omics Data Analysis\nGenomics, transcriptomics, proteomics, metabolomics, lipidomics, etc., uncovering the molecular mechanisms by studying their sequence information.\n\n\n02 Computational protein design\nDesigning functional proteins, peptides, enzymes, antibodies, nanobodies, etc.\n\n\n03 Computational micromolecule design\nDesigning small molecules that bind to protein targets, this includes inhibitors, activators, allosteric modulators, etc.\n\n\n04 Molecular dynamics\nComputer simulations to understand the molecular mechanisms of biological systems.\n\n\n05 Machine learning\nIn depth analysis of key machine learning models and their applications.\n\n\n06 New technologies\nNew technologies under investigation."
  },
  {
    "objectID": "01_0.html",
    "href": "01_0.html",
    "title": "Intro",
    "section": "",
    "text": "what’s the difference between genomics and transcriptomics? Genomics is the study of an organism’s entire genome, which includes all of its genes and their interactions. Transcriptomics is the study of all of the transcripts, or RNA molecules, that are produced from an organism’s genes. Transcriptomics provides a more detailed look at the expression of genes and how they are regulated, while genomics looks at the entire genome and how it is organized and functions.\nwhat’s the difference between proteomics and transcriptomics? Proteomics is the large-scale study of proteins, including their structure, function, and interactions, while transcriptomics is the study of transcriptomes, which are the complete set of RNA molecules, including mRNA, tRNA, and rRNA, produced by the transcription of DNA. Proteomics focuses on the proteins that are expressed in cells, while transcriptomics focuses on the transcripts that are produced and the role they play in gene expression."
  },
  {
    "objectID": "01_0.html#genomics",
    "href": "01_0.html#genomics",
    "title": "Intro",
    "section": "Genomics",
    "text": "Genomics\nGenomics is the study of an organism’s entire genome, which includes all of its genes and their interactions. Genomics is an interdisciplinary field that combines the techniques of genetics, molecular biology, bioinformatics, and computer science to study the structure, function, evolution, and regulation of genomes. Genomics is used to study the genetic makeup of organisms, including humans, plants, and animals, and to understand how genes are inherited and how they interact with each other and with the environment.\n\nPyro models of SARS-CoV-2 variants\nVariant calling and somatic mutation/CNV detection\nWGS variant calling using GATK"
  },
  {
    "objectID": "01_0.html#transcriptomics",
    "href": "01_0.html#transcriptomics",
    "title": "Intro",
    "section": "Transcriptomics",
    "text": "Transcriptomics"
  },
  {
    "objectID": "01_0.html#proteomics",
    "href": "01_0.html#proteomics",
    "title": "Intro",
    "section": "Proteomics",
    "text": "Proteomics"
  },
  {
    "objectID": "01_0.html#metagenomics",
    "href": "01_0.html#metagenomics",
    "title": "Intro",
    "section": "Metagenomics",
    "text": "Metagenomics\nMetagenomics is a field of study that focuses on the genomic analysis of entire microbial communities, typically including bacteria, archaea, and viruses, in an environmental sample. It is an interdisciplinary field that combines the techniques of genomics, bioinformatics, and ecology to study the collective genomes of microbial communities. Metagenomics can provide insight into the functions and interactions of microbial communities and their roles in various environments.\nwhat are the procedures of metagenomics?\n\nSample Collection: Metagenomics begins with the collection of an environmental sample, such as soil, water, or air.\nDNA Extraction: The DNA is then extracted from the sample, typically using a method such as lysis or centrifugation.\nLibrary Preparation: The extracted DNA is then prepared for sequencing in a process known as library preparation.\nSequencing: The prepared DNA is then sequenced using next-generation sequencing (NGS) or other methods.\nData Analysis: The resulting sequence data is then analyzed using bioinformatics tools and methods.\nResults Interpretation: The results of the analysis are then interpreted and used to draw conclusions about the microbial community and its role in the environment.\n\nwhat are the main developments in metagenomics in the last ten years?\n\nAdvances in sequencing technologies: Over the past decade, sequencing technologies have advanced significantly, creating a more cost-effective and efficient way to collect data on microbial communities. This has enabled researchers to generate unprecedented amounts of data, leading to new insights into the microbial world.\nBioinformatics advances: With the increase in sequencing data, the development of powerful bioinformatics tools has become critical to extracting meaningful information from metagenomic data. As a result, new analysis pipelines have been developed to compare metagenomes, detect patterns in metagenomic data, and identify novel microbial species.\nIncreased understanding of human microbiome: The development of metagenomics has enabled researchers to gain a better understanding of the human microbiome and its role in health and disease. This research has provided insights into how the microbiome contributes to various diseases and how it can be modified to improve human health.\nEnvironmental metagenomics: Metagenomics has also been used to explore microbial communities in the environment. This research has provided insights into how microbial communities interact with their environment, how they respond to environmental changes, and how they can be used to monitor environmental health."
  },
  {
    "objectID": "01_0.html#metabolomics",
    "href": "01_0.html#metabolomics",
    "title": "Intro",
    "section": "metabolomics",
    "text": "metabolomics\nMetabolomics is the study of the unique chemical fingerprints that specific cellular processes leave behind. It is an emerging field of study focused on the comprehensive analysis of the small molecule metabolites present in a biological sample, such as those from cells, tissues, or fluids. Through metabolomics, researchers can better understand the biochemical pathways and processes that are occurring in the body, providing insight into physiological states, disease processes, and responses to treatments."
  },
  {
    "objectID": "02_0.html",
    "href": "02_0.html",
    "title": "Intro",
    "section": "",
    "text": "Protein Structure Prediction using AlphaFold\nProtein Design using AlphaFold\nBinding energy upon mutation (ddG) Prediction for protein-protein complexes\nIlluminating protein space with a programmable generative model\nRobust deep learning based protein sequence design using MPNN\nIntegrating structure prediction networks and diffusion generative models"
  },
  {
    "objectID": "03_0.html",
    "href": "03_0.html",
    "title": "Intro",
    "section": "",
    "text": "Activators are small molecules that bind to a protein target and increase its activity. Allosteric modulators are small molecules that bind to a protein target and change its activity.\nEquivariant 3D-Conditional Diffusion Model for Molecular Linker Design"
  },
  {
    "objectID": "06_0.html",
    "href": "06_0.html",
    "title": "New technologies",
    "section": "",
    "text": "氘代技术"
  },
  {
    "objectID": "01_pyro_cov.html",
    "href": "01_pyro_cov.html",
    "title": "pyro_cov",
    "section": "",
    "text": "RSV and SARS-CoV-2 using Pyro\nnextstrain offers a nice way to download data from GISAID.\nusher is a tool to build a tree from a set of sequences.\ncov-lineages/pango-designation suggests new lineages that should be added to the current scheme.\ncov-lineages/pangoLEARN is a Store of the trained model for pangolin to access. This repository is deprecated and only for use with pangolin 2.0 and 3.0. For latest pangolin data models compatible with pangolin 4.0, use cov-lineages/pangolin-data, the repo for storing latest model, protobuf, designation hash and alias files for pangolin assignments\nCSSEGISandData/COVID-19 is a repository with Novel Coronavirus (COVID-19) Cases, provided by JHU CSSE.\nnextstrain/nextclade is a tool for Viral genome alignment, mutation calling, clade assignment, quality checks and phylogenetic placement\nManhattan plot of the mutation rates for the SARS-CoV-2 genome. The plot shows the log10 of the p-values for the null hypothesis that the mutation rate is zero. The plot is based on the results of the analysis of 6.4 million SARS-CoV-2 genomes. The plot shows that the mutation rate is significantly different from zero for 11 of the 29 genes in the SARS-CoV-2 genome. The genes with the highest mutation rates are ORF1ab, N, and S. The genes with the lowest mutation rates are E, M, and NSP3. The plot also shows that the mutation rate is significantly different from zero for the entire genome. The plot is based on the results of the analysis of 6.4 million SARS-CoV-2 genomes. The plot shows that the mutation rate is significantly different from zero for 11 of the 29 genes in the SARS-CoV-2 genome. The genes with the highest mutation rates are ORF1ab, N, and S. The genes with the lowest mutation rates are E, M, and NSP3. The plot also shows that the mutation rate is significantly different from zero for the entire genome.\n\nimport os\nimport sys\n\nREPO_ADDRESS = \"https://github.com/broadinstitute/pyro-cov.git\"\nREPO_NAME = \"pyro-cov\"\n\n# Download the repo if it doesn't exist\nif not os.path.exists(REPO_NAME):\n    !git clone $REPO_ADDRESS\n\n# change to the repo directory\nos.chdir(REPO_NAME)\n\n\n!pip install -e .\n\nDownload data\n\n# download the data\n!make update\n\nPreprocess data\nThis takes under an hour.\n\n!make preprocess\n\nanalyze data\n\n# analyze data\n!python scripts/mutrans.py --vary-gene"
  },
  {
    "objectID": "01_varscan.html",
    "href": "01_varscan.html",
    "title": "VarScan",
    "section": "",
    "text": "https://hpc.nih.gov/apps/VarScan.html"
  },
  {
    "objectID": "01_wgs.html",
    "href": "01_wgs.html",
    "title": "WGS",
    "section": "",
    "text": "clarify the choice of reference genome\nclarify the choice of known sites\nmodify the code to suit for multiple samples"
  },
  {
    "objectID": "01_wgs.html#tools",
    "href": "01_wgs.html#tools",
    "title": "WGS",
    "section": "Tools",
    "text": "Tools\nTools necessary for WGS variant calling using GATK include:\nPicard is a Java-based toolkit for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF.\nGATK is a toolkit for variant discovery in high-throughput sequencing data. It is designed to work best with data generated by next-generation DNA sequencing technologies, but can be applied to other types of data as well.\nSamtools is a suite of programs for interacting with high-throughput sequencing data. It provides various utilities for post-processing alignments in the SAM, BAM and CRAM formats, such as indexing, variant calling, viewing alignments, and generating alignments in a per-position format. It also includes a large number of utilities for manipulating alignments in a variety of different ways, for example, to extract all reads that map to a particular region of the genome.\nBWA is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the rest two for longer sequences ranged from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate. BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads. For all the algorithms, BWA first needs to construct the FM-index for the reference genome (the index command). Alignment algorithms are invoked with different sub-commands: aln/samse/sampe for BWA-backtrack, bwasw for BWA-SW and mem for the BWA-MEM algorithm.\nFastQC is a quality control tool for high throughput sequence data. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis. It produces a simple HTML file with a summary of the results of the analysis, which can be viewed in a web browser.\nOther tools that are useful for WGS variant calling include: - Freebayes, a Bayesian genetic variant detector designed to find small polymorphisms, specifically SNPs (single-nucleotide polymorphisms), indels (insertions and deletions), MNPs (multi-nucleotide polymorphisms), and complex events (composite insertion and substitution events) - bcftools, a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF - vcftools, a set of tools written in Perl and C++ for working with VCF files, such as those generated by the 1000 Genomes Project - bedtools, a suite of utilities for genome arithmetic - tabix, a generic indexer for TAB-delimited genome position files - bgzip, a small utility that enables tabix to work with gzip-compressed files"
  },
  {
    "objectID": "01_wgs.html#reference-genome",
    "href": "01_wgs.html#reference-genome",
    "title": "WGS",
    "section": "Reference genome",
    "text": "Reference genome\ndefine the reference genome directory and files\n\nimport os\n\nREF_DIR = \"/mnt/nas/wgs/hg38\"\nREF_URL = \"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\"\nREF_FILE = os.path.join(REF_DIR, \"hg38.fa\")\nKNOWN_SITES = os.path.join(REF_DIR, \"Homo_sapiens_assembly38.dbsnp138.vcf\")\nKNOWN_SITES_IDX = os.path.join(REF_DIR, \"Homos_sapiens_assembly38.dbsnp138.vcf.idx\")\n\ndownload reference genome and index it\n\n# Download the reference genome if the file doesn't exist\nif not os.path.exists(os.path.join(REF_DIR, \"hg38.fa\")):\n    !wget -O $REF_DIR/hg38.fa.gz $REF_URL\n    !gunzip $REF_DIR/hg38.fa.gz\n\n# index the reference genome file before running haplotype caller\nif not os.path.exists(os.path.join(REF_DIR, \"hg38.fa.fai\")):\n    !samtools faidx $REF_DIR/hg38.fa\n\n# Create dictionary file for the reference genome if it doesn't exist\nif not os.path.exists(os.path.join(REF_DIR, \"hg38.dict\")):\n    !gatk CreateSequenceDictionary R=$REF_DIR/hg38.fa O=$REF_DIR/hg38.dict\n\n# index the reference genome file using BWA, if the index files don't exist\nif not os.path.exists(os.path.join(REF_DIR, \"hg38.fa.sa\")):\n    !bwa index $REF_FILE\n\nSITE_URL = \"https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf\"\nSITE_IDX_URL = \"https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf.idx\"\n\n# Download known sites files for BQSR (Base Quality Score Recalibration) from GATK resource bundle if they don't exist\nif not os.path.exists(os.path.join(REF_DIR, \"Homo_sapiens_assembly38.dbsnp138.vcf\")):\n    !wget -O $REF_DIR $SITE_URL\n\nif not os.path.exists(os.path.join(REF_DIR, \"Homo_sapiens_assembly38.dbsnp138.vcf.idx\")):\n    !wget -O $REF_DIR $SITE_IDX_URL\n\n# show the contents of the reference directory\n!ls -halt $REF_DIR"
  },
  {
    "objectID": "01_wgs.html#reads",
    "href": "01_wgs.html#reads",
    "title": "WGS",
    "section": "Reads",
    "text": "Reads\n\nREADS_DIR = \"/mnt/nas/wgs/RDD2206P0001\"\nREADS_FILE1 = os.path.join(READS_DIR, \"RDD2206P0001-WGS-1_R1.fastq.gz\")\nREADS_FILE2 = os.path.join(READS_DIR, \"RDD2206P0001-WGS-1_R2.fastq.gz\")"
  },
  {
    "objectID": "01_wgs.html#quality-control",
    "href": "01_wgs.html#quality-control",
    "title": "WGS",
    "section": "Quality control",
    "text": "Quality control\nThis is a quick overview of the quality of the data. We will use FastQC to generate a report for each sample.\n\n# Run FastQC on the reads\n!fastqc $READS_FILE1 $READS_FILE2 -o $READS_DIR"
  },
  {
    "objectID": "01_wgs.html#align-reads-to-reference-genome",
    "href": "01_wgs.html#align-reads-to-reference-genome",
    "title": "WGS",
    "section": "Align reads to reference genome",
    "text": "Align reads to reference genome\nthe aligner bwa aln can trim the reads to remove low quality bases at the end of the reads. This is done using the -q option. The default value is 15.\nThe BWA-MEM algorithm performs local alignment. It may produce multiple primary alignments for different part of a query sequence. This is a crucial feature for long sequences. However, some tools such as Picard’s markDuplicates does not work with split alignments. One may consider to use option -M to flag shorter split hits as secondary. This option is not recommended for Illumina short reads.\n\n# Align the reads to the reference genome\n!bwa mem -t 50 -M -R \"@RG\\tID:RDD2206P0001\\tSM:RDD2206P0001\" $REF_FILE $READS_FILE1 $READS_FILE2 > $READS_DIR/RDD2206P0001-WGS-1.sam\n\n-R marks the complete read group header line. ’ can be used in STR and will be converted to a TAB in the output SAM. The read group ID will be attached to every read in the output. The read group ID is used to identify the read group in the output file. The read group sample name is used to identify the sample in the output file.\n\nView SAM file\nthe SAM file is a text file that contains the alignment of the reads to the reference genome. SAM stands for Sequence Alignment/Map format.\n\n# Show the first 10 lines of the SAM file\n!head -n 10 $READS_DIR/RDD2206P0001-WGS-1.sam\n\nWe can view the flag statistics of the SAM file using samtools flagstat command. The output of this command is a table that shows the number of reads that have a particular flag.\n\n# Show the flag statistics\n!samtools flagstat $READS_DIR/RDD2206P0001-WGS-1.sam\n\nThe SAM file contains the following flags:\n\n0x1: template having multiple segments in sequencing\n0x2: each segment properly aligned according to the aligner\n0x4: segment unmapped\n0x8: next segment in the template unmapped\n0x10: SEQ being reverse complemented\n0x20: SEQ of the next segment in the template being reversed\n0x40: the first segment in the template\n0x80: the last segment in the template\n0x100: secondary alignment\n0x200: not passing quality controls\n0x400: PCR or optical duplicate\n0x800: supplementary alignment\n\nThe SAM file contains the following fields:\n\nQNAME: Query template NAME\nFLAG: bitwise FLAG\nRNAME: Reference sequence NAME\nPOS: 1-based leftmost mapping POSition\nMAPQ: MAPping Quality\nCIGAR: CIGAR string\nRNEXT: Ref. name of the mate/next read\nPNEXT: Position of the mate/next read\nTLEN: observed Template LENgth\nSEQ: segment SEQuence\nQUAL: ASCII of Phred-scaled base QUALity+33\n\nThe SAM file needs to be sorted by the read name. This is done to make the file compatible with MarkDuplicatesSpark and HaplotypeCallerSpark."
  },
  {
    "objectID": "01_wgs.html#convert-sam-to-bam",
    "href": "01_wgs.html#convert-sam-to-bam",
    "title": "WGS",
    "section": "Convert SAM to BAM",
    "text": "Convert SAM to BAM\nthe BAM file is a binary file that contains the alignment of the reads to the reference genome. BAM stands for Binary Alignment/Map format. BAM is a compressed binary version of SAM.\n\n!samtools view -bS $READS_DIR/RDD2206P0001-WGS-1.sam > $READS_DIR/RDD2206P0001-WGS-1.bam"
  },
  {
    "objectID": "01_wgs.html#sort-bam",
    "href": "01_wgs.html#sort-bam",
    "title": "WGS",
    "section": "Sort BAM",
    "text": "Sort BAM\nThe BAM file is sorted by the read name. This is done to make the file compatible with MarkDuplicatesSpark and HaplotypeCallerSpark.\n\n!samtools sort -@ 50 $READS_DIR/RDD2206P0001-WGS-1.bam -o $READS_DIR/RDD2206P0001-WGS-1.sorted.bam\n\n\n# Show the flag statistics\n!samtools flagstat $READS_DIR/RDD2206P0001-WGS-1.sorted.bam"
  },
  {
    "objectID": "01_wgs.html#mark-duplicates",
    "href": "01_wgs.html#mark-duplicates",
    "title": "WGS",
    "section": "Mark duplicates",
    "text": "Mark duplicates\nMark duplicates in the BAM file. This is done to remove PCR duplicates. PCR duplicates are reads that are generated from the same DNA fragment. PCR duplicates are generated during the PCR amplification step. PCR duplicates are removed to avoid overestimation of the coverage of the genome.\nMarkDuplicatesSpark is a Spark version of MarkDuplicates. It is faster than MarkDuplicates and can be run on a cluster.\n\n!gatk MarkDuplicatesSpark -I $READS_DIR/RDD2206P0001-WGS-1.sorted.bam -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam -M $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.metrics\n\n\n# Show the flag statistics\n!samtools flagstat $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam"
  },
  {
    "objectID": "01_wgs.html#index-bam",
    "href": "01_wgs.html#index-bam",
    "title": "WGS",
    "section": "Index BAM",
    "text": "Index BAM\nIndex the BAM file. This is done to make the file compatible with HaplotypeCallerSpark.\n\n!samtools index $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam"
  },
  {
    "objectID": "01_wgs.html#base-quality-score-recalibration",
    "href": "01_wgs.html#base-quality-score-recalibration",
    "title": "WGS",
    "section": "Base quality score recalibration",
    "text": "Base quality score recalibration\nBase quality score recalibration (BQSR) is a process that recalibrates the base quality scores of the reads. BQSR is done to correct for systematic errors in the base quality scores. BQSR is done using the BaseRecalibrator and ApplyBQSR tools. The BaseRecalibrator tool generates a recalibration table. The ApplyBQSR tool applies the recalibration table to the BAM file.\n\n!gatk BaseRecalibrator -I $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam -R $REF_FILE -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal_data.table --known-sites $KNOWN_SITES\n!gatk ApplyBQSR -R $REF_FILE -I $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.bam --bqsr-recal-file $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal_data.table -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.bam"
  },
  {
    "objectID": "01_wgs.html#haplotype-caller",
    "href": "01_wgs.html#haplotype-caller",
    "title": "WGS",
    "section": "Haplotype caller",
    "text": "Haplotype caller\nThe HaplotypeCaller tool is used to call variants. The HaplotypeCaller tool is a Spark version of the HaplotypeCaller tool. It is faster than the HaplotypeCaller tool and can be run on a cluster.\n\n!gatk HaplotypeCaller -R $REF_FILE -I $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.bam -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.vcf"
  },
  {
    "objectID": "01_wgs.html#variant-filtering",
    "href": "01_wgs.html#variant-filtering",
    "title": "WGS",
    "section": "Variant filtering",
    "text": "Variant filtering\n\n!gatk VariantFiltration -R $REF_FILE -V $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.vcf -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.vcf --filter-expression \"QD < 2.0\" --filter-name \"QD2\" --filter-expression \"FS > 60.0\" --filter-name \"FS60\" --filter-expression \"MQ < 40"
  },
  {
    "objectID": "01_wgs.html#variant-annotation",
    "href": "01_wgs.html#variant-annotation",
    "title": "WGS",
    "section": "Variant annotation",
    "text": "Variant annotation\n\n!gatk VariantAnnotator -R $REF_FILE -V $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.vcf -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.vcf --dbsnp $KNOWN_SITES"
  },
  {
    "objectID": "01_wgs.html#variant-recalibration",
    "href": "01_wgs.html#variant-recalibration",
    "title": "WGS",
    "section": "Variant recalibration",
    "text": "Variant recalibration\n\n!gatk VariantRecalibrator -R $REF_FILE -V $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.vcf -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.vcf --tranches-file $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.tranches --rscript-file $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.plots.R --resource:hapmap,known=false,training=true,truth=true,prior=15.0 $HAPMAP --resource:omni,known=false,training=true,truth=false,prior=12.0 $OMNI --resource:1000G,known=false,training=true,truth=false,prior=10.0 $KG --resource:dbsnp,known=true,training=false,truth=false,prior=2.0 $KNOWN_SITES --an QD --an MQ --an MQRankSum --an ReadPosRankSum --an FS --an SOR --mode SNP --tranche 100.0 --tranche 99.9 --tranche 99.0 --tranche 90.0 --max-gaussians 4"
  },
  {
    "objectID": "01_wgs.html#apply-variant-recalibration",
    "href": "01_wgs.html#apply-variant-recalibration",
    "title": "WGS",
    "section": "Apply variant recalibration",
    "text": "Apply variant recalibration\n\n!gatk ApplyVQSR -R $REF_FILE -V $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.vcf -O $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.vcf --ts-filter-level 99.0 --tranches-file $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.tranches --recal-file $READS_DIR/RDD2206P0001-WGS-1.sorted.dedup.recal.filtered.annotated.recal.vcf --mode SNP"
  },
  {
    "objectID": "02_AFdesign.html",
    "href": "02_AFdesign.html",
    "title": "AFdesign",
    "section": "",
    "text": "Fixed backbone design\nFor a given protein backbone, generate/design a new sequence that AlphaFold thinks folds into that conformation.\n\nclear_mem()\naf_model = mk_afdesign_model(protocol=\"fixbb\")\naf_model.prep_inputs(pdb_filename=download_pdb(\"1TEN\"), chain=\"A\")\n\nprint(\"length\",  af_model._len)\nprint(\"weights\", af_model.opt[\"weights\"])\n\n\naf_model.restart()\naf_model.design_3stage()\n\n\naf_model.plot_traj()  \n\nThe plot_traj function plots the training trajectories.\n\naf_model.save_pdb(f\"tenascin_{af_model.protocol}.pdb\")\n\n\nHTML(af_model.animate())\n\n\naf_model.get_seqs()\n\n\n\nHallucination"
  },
  {
    "objectID": "02_AlphaFold.html",
    "href": "02_AlphaFold.html",
    "title": "AlphaFold",
    "section": "",
    "text": "Other folding libraries:\n\nhttps://github.com/aqlaboratory/openfold\nhttps://github.com/sokrypton/ColabFold\nhttps://github.com/RosettaCommons/RoseTTAFold\nhttps://github.com/HeliXonProtein/OmegaFold\n\nESM merits its own treatment: https://github.com/facebookresearch/esm\n\nDownload the code\n\nimport os\nimport sys\n\nREPO_ADDRESS = \"https://github.com/deepmind/alphafold.git\"\nREPO_NAME = \"alphafold\"\n\nif not os.path.exists(REPO_NAME):\n    !git clone {REPO_ADDRESS}\n\n%cd {REPO_NAME}\n\nCloning into 'alphafold'...\nremote: Enumerating objects: 767, done.\nremote: Counting objects: 100% (133/133), done.\nremote: Compressing objects: 100% (81/81), done.\nremote: Total 767 (delta 59), reused 73 (delta 44), pack-reused 634\nReceiving objects: 100% (767/767), 16.08 MiB | 12.14 MiB/s, done.\nResolving deltas: 100% (433/433), done.\n/home/ma/git/computation/alphafold\n\n\n\n\nThe model parameters\nDeepmind updates the model parameters every 6 months. The latest model parameters are available at the following link:\nhttps://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar\n\nPARAMS_ADDRESS = \"https://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar\"\nPARAMS_NAME = \"alphafold_params_2022-12-06.tar\"\nPARAMS_DIR = \"/mnt/nas/alphafold/alphafold_params_2022-12-06\"\n\n\n# download params if not already downloaded, create directory if not already created, including parent directories\nif not os.path.exists(PARAMS_DIR):\n    !mkdir -p {PARAMS_DIR}\n    !wget {PARAMS_ADDRESS} -P {PARAMS_DIR}\n    !tar -xvf {PARAMS_DIR}/{PARAMS_NAME} -C {PARAMS_DIR}\n\n\n\nThe data\nThe data will be downloaded separately, since it’s not a task to be done in a single notebook session."
  },
  {
    "objectID": "02_binding_ddg.html",
    "href": "02_binding_ddg.html",
    "title": "binding_ddG",
    "section": "",
    "text": "# suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nMUT_PROTEIN = \"data/example_mut.pdb\"\nWT_PROTEIN = \"data/example_wt.pdb\"\n\n\n# show the protein chains and number of amino acids\ndef show_protein(protein):\n    from Bio.PDB import PDBParser\n\n    parser = PDBParser()\n    structure = parser.get_structure(\"protein\", protein)\n    for model in structure:\n        for chain in model:\n            print(chain.id, len(chain))\n\nshow_protein(MUT_PROTEIN)\nshow_protein(WT_PROTEIN)\n\nA 214\nB 214\nC 200\nA 214\nB 214\nC 200\n\n\n\n\n# Get Amino Acid sequence from PDB file\ndef get_sequence(pdb_file, chain=\"A\"):\n    from Bio.PDB import PDBParser, PPBuilder\n\n    parser = PDBParser()\n    structure = parser.get_structure(\"protein\", pdb_file)\n    ppb = PPBuilder()\n    for pp in ppb.build_peptides(structure[0][chain]):\n        return pp.get_sequence()\n\n# Get sequence of mutated protein\nmut_seq = get_sequence(MUT_PROTEIN)\n# Get sequence of wild type protein\nwt_seq = get_sequence(WT_PROTEIN)\n\n# Show sequences\nprint(\"Mutated protein sequence: \", mut_seq)\nprint(\"Wild type protein sequence: \", wt_seq)\n\nMutated protein sequence:  DIKMTQSPSSMYASLGERVTITCKASQDIRKYLNWYQQKPWKSPKTLIYYATSLADGVPSRFSGSGSGQDYSLTISSLESDDTATYYCLQHGESPYTFGGGTKLEINRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC\nWild type protein sequence:  DIKMTQSPSSMYASLGERVTITCKASQDIRKYLNWYQQKPWKSPKTLIYYATSLADGVPSRFSGSGSGQDYSLTISSLESDDTATYYCLQHGESPYTFGGGTKLEINRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC\n\n\n\n# Show difference between sequences\ndef show_aa_diff(seq1, seq2):\n    for i in range(len(seq1)):\n        if seq1[i] != seq2[i]:\n            print(f\"Position {i+1}: {seq1[i]} -> {seq2[i]}\")\n\n\nseq1 = \"ABCD\"\nseq2 = \"ABEF\"\nshow_aa_diff(seq1, seq2)\n\nPosition 3: C -> E\nPosition 4: D -> F\n\n\n\n\n# loop through the peptide chains to find the mutations\nfor chain in \"ABC\":\n    print(f\"Chain {chain}:\")\n    show_aa_diff(get_sequence(MUT_PROTEIN, chain), get_sequence(WT_PROTEIN, chain))\n\nChain A:\nChain B:\nChain C:\n\n\n\n# compute the ddG\n!python scripts/predict.py $WT_PROTEIN $MUT_PROTEIN\n\nPredicted ddG: -0.30"
  },
  {
    "objectID": "02_chroma.html",
    "href": "02_chroma.html",
    "title": "chroma",
    "section": "",
    "text": "https://generatebiomedicines.com/chroma\nhttps://github.com/lucidrains/chroma-pytorch"
  },
  {
    "objectID": "02_MPNN.html",
    "href": "02_MPNN.html",
    "title": "ProteinMPNN",
    "section": "",
    "text": "import os\n\nREPO_ADDRESS = \"https://github.com/dauparas/ProteinMPNN.git\"\n# Download GitHub repo if not already downloaded\nif not os.path.exists(\"ProteinMPNN\"):\n    !git clone $REPO_ADDRESS\n\n# Change working directory to repo\nos.chdir(\"ProteinMPNN\")\n\nimport\n\nimport matplotlib.pyplot as plt\nimport shutil\nimport warnings\nimport numpy as np\nimport torch\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split, Subset\nimport copy\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\n\n\nfrom protein_mpnn_utils import loss_nll, loss_smoothed, gather_edges, gather_nodes, gather_nodes_t, cat_neighbors_nodes, _scores, _S_to_seq, tied_featurize, parse_PDB\nfrom protein_mpnn_utils import StructureDataset, StructureDatasetPDB, ProteinMPNN\n\nmodel specification\n\n\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n#v_48_010=version with 48 edges 0.10A noise\nmodel_name = \"v_48_020\" #@param [\"v_48_002\", \"v_48_010\", \"v_48_020\", \"v_48_030\"]\npath_to_model_weights='vanilla_model_weights'          \nmodel_folder_path = path_to_model_weights\n\nif model_folder_path[-1] != '/':\n    model_folder_path = model_folder_path + '/'\n\ncheckpoint_path = model_folder_path + f'{model_name}.pt'\ncheckpoint = torch.load(checkpoint_path, map_location=device) \nprint('Number of edges:', checkpoint['num_edges'])\nnoise_level_print = checkpoint['noise_level']\nprint(f'Training noise level: {noise_level_print}A')\n\nNumber of edges: 48\nTraining noise level: 0.2A\n\n\n\nbackbone_noise=0.00               # Standard deviation of Gaussian noise to add to backbone atoms\nhidden_dim = 128\nnum_layers = 3 \n\nmodel = ProteinMPNN(\n    num_letters=21, \n    node_features=hidden_dim, \n    edge_features=hidden_dim, \n    hidden_dim=hidden_dim, \n    num_encoder_layers=num_layers, \n    num_decoder_layers=num_layers, \n    augment_eps=backbone_noise, \n    k_neighbors=checkpoint['num_edges'])\nmodel.to(device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\nprint(\"Model loaded\")\n\nModel loaded\n\n\nHelper function to tie weights of homomer chains\nExamples: 1) pdb: 6MRR, homomer: False, designed_chain: A 2) pdb: 1O91, homomer: True, designed_chain: A B C, for correct symmetric tying lenghts of homomer chains should be the same\n\ndef make_tied_positions_for_homomers(pdb_dict_list):\n    my_dict = {}\n    for result in pdb_dict_list:\n        all_chain_list = sorted([item[-1:] for item in list(result) if item[:9]=='seq_chain']) #A, B, C, ...\n        tied_positions_list = []\n        chain_length = len(result[f\"seq_chain_{all_chain_list[0]}\"])\n        for i in range(1,chain_length+1):\n            temp_dict = {}\n            for j, chain in enumerate(all_chain_list):\n                temp_dict[chain] = [i] #needs to be a list\n            tied_positions_list.append(temp_dict)\n        my_dict[result['name']] = tied_positions_list\n    return my_dict\n\n\nimport re\nimport numpy as np\n\n# Download PDB file from RCSB, given a PDB ID\ndef get_pdb(pdb_id):\n    pdb_id = pdb_id.lower()\n    pdb_filename = f\"{pdb_id}.pdb\"\n    if not os.path.exists(pdb_filename):\n        !wget https://files.rcsb.org/download/{pdb_filename}\n    return pdb_filename\n\npdb='1O91'\npdb_path = get_pdb(pdb)\n\n\nhomomer = True #@param {type:\"boolean\"}\ndesigned_chain = \"A B C\" #@param {type:\"string\"}\nfixed_chain = \"\" #@param {type:\"string\"}\n\nif designed_chain == \"\":\n  designed_chain_list = []\nelse:\n  designed_chain_list = re.sub(\"[^A-Za-z]+\",\",\", designed_chain).split(\",\")\n\nif fixed_chain == \"\":\n  fixed_chain_list = []\nelse:\n  fixed_chain_list = re.sub(\"[^A-Za-z]+\",\",\", fixed_chain).split(\",\")\n\nchain_list = list(set(designed_chain_list + fixed_chain_list))\n\n#@markdown - specified which chain(s) to design and which chain(s) to keep fixed. \n#@markdown   Use comma:`A,B` to specifiy more than one chain\n\n#chain = \"A\" #@param {type:\"string\"}\n#pdb_path_chains = chain\n##@markdown - Define which chain to redesign\n\n#@markdown ### Design Options\nnum_seqs = 1 #@param [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\", \"64\"] {type:\"raw\"}\nnum_seq_per_target = num_seqs\n\n#@markdown - Sampling temperature for amino acids, T=0.0 means taking argmax, T>>1.0 means sample randomly.\nsampling_temp = \"0.1\" #@param [\"0.0001\", \"0.1\", \"0.15\", \"0.2\", \"0.25\", \"0.3\", \"0.5\"]\n\n\nsave_score=0                      # 0 for False, 1 for True; save score=-log_prob to npy files\nsave_probs=0                      # 0 for False, 1 for True; save MPNN predicted probabilites per position\nscore_only=0                      # 0 for False, 1 for True; score input backbone-sequence pairs\nconditional_probs_only=0          # 0 for False, 1 for True; output conditional probabilities p(s_i given the rest of the sequence and backbone)\nconditional_probs_only_backbone=0 # 0 for False, 1 for True; if true output conditional probabilities p(s_i given backbone)\n    \nbatch_size=1                      # Batch size; can set higher for titan, quadro GPUs, reduce this if running out of GPU memory\nmax_length=20000                  # Max sequence length\n    \njsonl_path=''                     # Path to a folder with parsed pdb into jsonl\nomit_AAs='X'                      # Specify which amino acids should be omitted in the generated sequence, e.g. 'AC' would omit alanine and cystine.\n   \npssm_multi=0.0                    # A value between [0.0, 1.0], 0.0 means do not use pssm, 1.0 ignore MPNN predictions\npssm_threshold=0.0                # A value between -inf + inf to restric per position AAs\npssm_log_odds_flag=0               # 0 for False, 1 for True\npssm_bias_flag=0                   # 0 for False, 1 for True\n\n\n\nout_folder='.'                    # Path to a folder to output sequences, e.g. /home/out/\nfolder_for_outputs = out_folder\n\nNUM_BATCHES = num_seq_per_target//batch_size\nBATCH_COPIES = batch_size\ntemperatures = [float(item) for item in sampling_temp.split()]\nomit_AAs_list = omit_AAs\nalphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n\nomit_AAs_np = np.array([AA in omit_AAs_list for AA in alphabet]).astype(np.float32)\n\nchain_id_dict = None\nfixed_positions_dict = None\npssm_dict = None\nomit_AA_dict = None\nbias_AA_dict = None\ntied_positions_dict = None\nbias_by_res_dict = None\nbias_AAs_np = np.zeros(len(alphabet))\n\n\n\npdb_dict_list = parse_PDB(pdb_path, input_chain_list=chain_list)\ndataset_valid = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=max_length)\n\nchain_id_dict = {}\nchain_id_dict[pdb_dict_list[0]['name']]= (designed_chain_list, fixed_chain_list)\n\nprint(chain_id_dict)\nfor chain in chain_list:\n  l = len(pdb_dict_list[0][f\"seq_chain_{chain}\"])\n  print(f\"Length of chain {chain} is {l}\")\n\nif homomer:\n  tied_positions_dict = make_tied_positions_for_homomers(pdb_dict_list)\nelse:\n  tied_positions_dict = None\n\n{'1o91': (['A', 'B', 'C'], [])}\nLength of chain B is 131\nLength of chain A is 131\nLength of chain C is 131\n\n\nrun prediction\n\nwith torch.no_grad():\n  print('Generating sequences...')\n  for ix, protein in enumerate(dataset_valid):\n    score_list = []\n    all_probs_list = []\n    all_log_probs_list = []\n    S_sample_list = []\n    batch_clones = [copy.deepcopy(protein) for i in range(BATCH_COPIES)]\n    X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta = tied_featurize(batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict, pssm_dict, bias_by_res_dict)\n    pssm_log_odds_mask = (pssm_log_odds_all > pssm_threshold).float() #1.0 for true, 0.0 for false\n    name_ = batch_clones[0]['name']\n\n    randn_1 = torch.randn(chain_M.shape, device=X.device)\n    log_probs = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n    mask_for_loss = mask*chain_M*chain_M_pos\n    scores = _scores(S, log_probs, mask_for_loss)\n    native_score = scores.cpu().data.numpy()\n\n    for temp in temperatures:\n        for j in range(NUM_BATCHES):\n            randn_2 = torch.randn(chain_M.shape, device=X.device)\n            if tied_positions_dict == None:\n                sample_dict = model.sample(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), bias_by_res=bias_by_res_all)\n                S_sample = sample_dict[\"S\"] \n            else:\n                sample_dict = model.tied_sample(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), tied_pos=tied_pos_list_of_lists_list[0], tied_beta=tied_beta, bias_by_res=bias_by_res_all)\n            # Compute scores\n                S_sample = sample_dict[\"S\"]\n            log_probs = model(X, S_sample, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_2, use_input_decoding_order=True, decoding_order=sample_dict[\"decoding_order\"])\n            mask_for_loss = mask*chain_M*chain_M_pos\n            scores = _scores(S_sample, log_probs, mask_for_loss)\n            scores = scores.cpu().data.numpy()\n            all_probs_list.append(sample_dict[\"probs\"].cpu().data.numpy())\n            all_log_probs_list.append(log_probs.cpu().data.numpy())\n            S_sample_list.append(S_sample.cpu().data.numpy())\n            for b_ix in range(BATCH_COPIES):\n                masked_chain_length_list = masked_chain_length_list_list[b_ix]\n                masked_list = masked_list_list[b_ix]\n                seq_recovery_rate = torch.sum(torch.sum(torch.nn.functional.one_hot(S[b_ix], 21)*torch.nn.functional.one_hot(S_sample[b_ix], 21),axis=-1)*mask_for_loss[b_ix])/torch.sum(mask_for_loss[b_ix])\n                seq = _S_to_seq(S_sample[b_ix], chain_M[b_ix])\n                score = scores[b_ix]\n                score_list.append(score)\n                native_seq = _S_to_seq(S[b_ix], chain_M[b_ix])\n                if b_ix == 0 and j==0 and temp==temperatures[0]:\n                    start = 0\n                    end = 0\n                    list_of_AAs = []\n                    for mask_l in masked_chain_length_list:\n                        end += mask_l\n                        list_of_AAs.append(native_seq[start:end])\n                        start = end\n                    native_seq = \"\".join(list(np.array(list_of_AAs)[np.argsort(masked_list)]))\n                    l0 = 0\n                    for mc_length in list(np.array(masked_chain_length_list)[np.argsort(masked_list)])[:-1]:\n                        l0 += mc_length\n                        native_seq = native_seq[:l0] + '/' + native_seq[l0:]\n                        l0 += 1\n                    sorted_masked_chain_letters = np.argsort(masked_list_list[0])\n                    print_masked_chains = [masked_list_list[0][i] for i in sorted_masked_chain_letters]\n                    sorted_visible_chain_letters = np.argsort(visible_list_list[0])\n                    print_visible_chains = [visible_list_list[0][i] for i in sorted_visible_chain_letters]\n                    native_score_print = np.format_float_positional(np.float32(native_score.mean()), unique=False, precision=4)\n                    line = '>{}, score={}, fixed_chains={}, designed_chains={}, model_name={}\\n{}\\n'.format(name_, native_score_print, print_visible_chains, print_masked_chains, model_name, native_seq)\n                    print(line.rstrip())\n                start = 0\n                end = 0\n                list_of_AAs = []\n                for mask_l in masked_chain_length_list:\n                    end += mask_l\n                    list_of_AAs.append(seq[start:end])\n                    start = end\n\n                seq = \"\".join(list(np.array(list_of_AAs)[np.argsort(masked_list)]))\n                l0 = 0\n                for mc_length in list(np.array(masked_chain_length_list)[np.argsort(masked_list)])[:-1]:\n                    l0 += mc_length\n                    seq = seq[:l0] + '/' + seq[l0:]\n                    l0 += 1\n                score_print = np.format_float_positional(np.float32(score), unique=False, precision=4)\n                seq_rec_print = np.format_float_positional(np.float32(seq_recovery_rate.detach().cpu().numpy()), unique=False, precision=4)\n                line = '>T={}, sample={}, score={}, seq_recovery={}\\n{}\\n'.format(temp,b_ix,score_print,seq_rec_print,seq)\n                print(line.rstrip())\n\n\nGenerating sequences...\n>1o91, score=1.2937, fixed_chains=[], designed_chains=['A', 'B', 'C'], model_name=v_48_020\nEMPAFTAELTVPFPPVGAPVKFDKLLYNGRQNYNPQTGIFTCEVPGVYYFAYHVHCKGGNVWVALFKNNEPMMYTYDEYKKGFLDQASGSAVLLLRPGDQVFLQMPSEQAAGLYAGQYVHSSFSGYLLYPM/EMPAFTAELTVPFPPVGAPVKFDKLLYNGRQNYNPQTGIFTCEVPGVYYFAYHVHCKGGNVWVALFKNNEPMMYTYDEYKKGFLDQASGSAVLLLRPGDQVFLQMPSEQAAGLYAGQYVHSSFSGYLLYPM/EMPAFTAELTVPFPPVGAPVKFDKLLYNGRQNYNPQTGIFTCEVPGVYYFAYHVHCKGGNVWVALFKNNEPMMYTYDEYKKGFLDQASGSAVLLLRPGDQVFLQMPSEQAAGLYAGQYVHSSFSGYLLYPM\n>T=0.1, sample=0, score=0.6260, seq_recovery=0.4733\nEVEAFTALLTTPNPAVGTPIKFDKVVYNGQNVYDPATGIFTVKTPGIYFFTWVLYVYGNDLLAELMKNDTPVMKVYLQNVDGKINQVSGAAILELKEGDKVYIKIPSSSANGLYASATNHSYFSGYLLKEL/EVEAFTALLTTPNPAVGTPIKFDKVVYNGQNVYDPATGIFTVKTPGIYFFTWVLYVYGNDLLAELMKNDTPVMKVYLQNVDGKINQVSGAAILELKEGDKVYIKIPSSSANGLYASATNHSYFSGYLLKEL/EVEAFTALLTTPNPAVGTPIKFDKVVYNGQNVYDPATGIFTVKTPGIYFFTWVLYVYGNDLLAELMKNDTPVMKVYLQNVDGKINQVSGAAILELKEGDKVYIKIPSSSANGLYASATNHSYFSGYLLKEL\n\n\n\n\nall_probs_concat = np.concatenate(all_probs_list)\nall_log_probs_concat = np.concatenate(all_log_probs_list)\nS_sample_concat = np.concatenate(S_sample_list)\n\nAmino acid probabilties\n\nimport plotly.express as px\nfig = px.imshow(np.exp(all_log_probs_concat).mean(0).T,\n                labels=dict(x=\"positions\", y=\"amino acids\", color=\"probability\"),\n                y=list(alphabet),\n                template=\"simple_white\"\n               )\n\nfig.update_xaxes(side=\"top\")\n\n\nfig.show()\n\n\n                                                \n\n\nSampling temperature adjusted amino acid probabilties\n\nimport plotly.express as px\nfig = px.imshow(all_probs_concat.mean(0).T,\n                labels=dict(x=\"positions\", y=\"amino acids\", color=\"probability\"),\n                y=list(alphabet),\n                template=\"simple_white\"\n               )\n\nfig.update_xaxes(side=\"top\")\n\n\nfig.show()"
  },
  {
    "objectID": "02_RFdiffusion.html",
    "href": "02_RFdiffusion.html",
    "title": "RFdiffusion",
    "section": "",
    "text": "Protein binder design\n\n\nSymmetric oligomer design\nsymmetric oligomers can be used to\n\n\nEnzyme active site scaffolding\nenzyme active sites are\n\n\nSymmetric motif scaffolding (e.g. helix, beta sheet) design\nSymmetric motif scaffolds are designed by specifying the sequence of the motif and the number of repeats. The motif is then repeated in the sequence of the scaffold. The motif is specified by a list of amino acids, where each amino acid is specified by a one-letter code. The number of repeats is specified by an integer."
  },
  {
    "objectID": "03_difflinker.html",
    "href": "03_difflinker.html",
    "title": "DiffLinker",
    "section": "",
    "text": "import os\nimport sys\n\nREPO_ADDRESS = \"https://github.com/igashov/DiffLinker.git\"\nREPO_NAME = \"DiffLinker\"\n\nif not os.path.exists(REPO_NAME):\n    !git clone {REPO_ADDRESS}\n\n%cd {REPO_NAME}\n\nCloning into 'DiffLinker'...\nremote: Enumerating objects: 185, done.\nremote: Counting objects: 100% (185/185), done.\nremote: Compressing objects: 100% (136/136), done.\nremote: Total 185 (delta 81), reused 139 (delta 44), pack-reused 0\nReceiving objects: 100% (185/185), 17.92 MiB | 13.56 MiB/s, done.\nResolving deltas: 100% (81/81), done.\n/home/ma/git/computation/DiffLinker"
  },
  {
    "objectID": "05_BioGPT.html",
    "href": "05_BioGPT.html",
    "title": "Scientific Computation at large",
    "section": "",
    "text": "The BioGPT-Large model with 1.5B paramters hosted at huggingface.\n\nimport torch\nfrom transformers import BioGptTokenizer, BioGptForCausalLM, set_seed, pipeline\n\n\ntokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\nmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n\nBeam-search decoding. The beam size determines the number of hypotheses to consider at each step. The higher the beam size, the more accurate the results, but the slower the inference.\n\n\nset_seed(43)\nsentence = \"to differentiate germline and sporadic mutations, we need:\"\ninputs = tokenizer(sentence, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    beam_output = model.generate(**inputs,\n                                min_length=100,\n                                max_length=1024,\n                                num_beams=10,\n                                early_stopping=True\n                                )\ntokenizer.decode(beam_output[0], skip_special_tokens=True)\n\n'to differentiate germline and sporadic mutations, we need: (1) a better understanding of the mechanisms of germline mutation; (2) a better understanding of the mechanisms of somatic mutation; (3) a better understanding of the mechanisms of somatic mutation; (4) a better understanding of the mechanisms of germline mutation; (5) a better understanding of the mechanisms of somatic mutation; (6) a better understanding of the mechanisms of germline mutation; (7) a better understanding of the mechanisms of somatic mutation; and (8) a better understanding of the mechanisms of germline mutation.'\n\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:\n\ngenerator = pipeline('text-generation', model=model, tokenizer=tokenizer)\ngenerator(sentence, max_length=50, num_return_sequences=1, do_sample=True)\n\n[{'generated_text': 'to differentiate germline and sporadic mutations, we need: (1) methods / assays not exclusively focused on germline ones such as the identification of new disease-associated single nucleotide variants and next-generation sequencing strategies able to characterize non-inherited disease-causing variants'}]\n\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\nfrom transformers import BioGptTokenizer, BioGptForCausalLM\ntokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\nmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)"
  },
  {
    "objectID": "05_dnn.html",
    "href": "05_dnn.html",
    "title": "Neural network: layers and architectures",
    "section": "",
    "text": "In this notebook we’ll introduce the basic layers and architectures used in deep neural networks.\nThe code will be written in PyTorch, but the concepts are applicable to any deep learning framework."
  },
  {
    "objectID": "05_dnn.html#linear-layers",
    "href": "05_dnn.html#linear-layers",
    "title": "Neural network: layers and architectures",
    "section": "Linear layers",
    "text": "Linear layers\nA linear layer is a layer that applies a linear transformation to its input. It is defined by a weight matrix and a bias vector. The output is computed as:\n\\[y = f(x; \\theta) = Wx + b\\]\nwhere \\(x\\) is the input, \\(W\\) is the weight matrix, \\(b\\) is the bias vector.\nThe linear layer is implemented in PyTorch as F.linear.\n\n\nCode\n# define a linear layer in NumPy\ndef linear_layer(x, w, b):\n    return x @ w + b\n\n\n\n\nCode\nin_features = 28 * 28\nout_features = 10\n\n# test the linear layer\nx = torch.randn(1, in_features)\nw = torch.randn(in_features, out_features)\nb = torch.randn(out_features)\n\n# compare the output of the NumPy implementation with the PyTorch implementation\nassert torch.allclose(linear_layer(x, w, b), F.linear(x, w.T, b))\n\n\nnote that in PyTorch the weight matrix has the shape (out_features, in_features) so we have to transpose it."
  },
  {
    "objectID": "05_dnn.html#non-linear-activations",
    "href": "05_dnn.html#non-linear-activations",
    "title": "Neural network: layers and architectures",
    "section": "Non-linear activations",
    "text": "Non-linear activations\nIt’s useless to have deep neural networks with only linear layers because they can be replaced by a single linear layer. We need to add non-linear activations to the network to make it more powerful.\n\nGELU\nThe GELU activation function, which stands for Gaussian Error Linear Unit, is a popular activation function used in neural networks. It was first introduced by Dan Hendrycks and Kevin Gimpel in their paper “Gaussian Error Linear Units (GELUs)” in 2017.\nThe GELU activation function is defined as follows:\n\\[GELU(x) = x \\Phi(x) = 0.5 x (1 + erf(x/\\sqrt{2})) \\]\nwhere \\(\\Phi\\) is the cumulative distribution function of the standard normal distribution and erf is the error function.\nThe GELU activation function has a similar shape to the widely used ReLU activation function, but with some key differences. One of the main advantages of GELU over ReLU is that it has a non-zero mean, which can help to reduce the vanishing gradient problem. Additionally, GELU has been shown to outperform other activation functions in certain scenarios, such as on language modeling tasks.\nHowever, it should be noted that GELU is a relatively new activation function and may not always be the best choice for every application. As with any neural network component, it is important to experiment with different activation functions to find the one that works best for your specific problem.\n\n\nCode\n# define a GELU layer in NumPy\ndef gelu_layer(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n\n\n\n\nCode\n# test the GELU layer\nx = torch.randn(1, 2)\n\nprint(gelu_layer(x), F.gelu(x))\n\n\ntensor([[-0.1595, -0.1053]]) tensor([[-0.1594, -0.1053]])\n\n\nThe two implementations do not always give the same results because the torch.erf function is not as precise as the scipy.special.erf function."
  },
  {
    "objectID": "05_dnn.html#conv-layers",
    "href": "05_dnn.html#conv-layers",
    "title": "Neural network: layers and architectures",
    "section": "Conv layers",
    "text": "Conv layers"
  },
  {
    "objectID": "05_dnn.html#resiudal-blocks",
    "href": "05_dnn.html#resiudal-blocks",
    "title": "Neural network: layers and architectures",
    "section": "resiudal blocks",
    "text": "resiudal blocks"
  },
  {
    "objectID": "05_dnn.html#normalization-layers",
    "href": "05_dnn.html#normalization-layers",
    "title": "Neural network: layers and architectures",
    "section": "Normalization layers",
    "text": "Normalization layers"
  },
  {
    "objectID": "05_dnn.html#dropout",
    "href": "05_dnn.html#dropout",
    "title": "Neural network: layers and architectures",
    "section": "Dropout",
    "text": "Dropout"
  },
  {
    "objectID": "05_dnn.html#attention",
    "href": "05_dnn.html#attention",
    "title": "Neural network: layers and architectures",
    "section": "Attention",
    "text": "Attention"
  },
  {
    "objectID": "05_dnn.html#recurrent-layers",
    "href": "05_dnn.html#recurrent-layers",
    "title": "Neural network: layers and architectures",
    "section": "Recurrent layers",
    "text": "Recurrent layers"
  },
  {
    "objectID": "05_dnn.html#multiplicative-layers",
    "href": "05_dnn.html#multiplicative-layers",
    "title": "Neural network: layers and architectures",
    "section": "Multiplicative layers",
    "text": "Multiplicative layers"
  },
  {
    "objectID": "05_dnn.html#implicit-layers",
    "href": "05_dnn.html#implicit-layers",
    "title": "Neural network: layers and architectures",
    "section": "Implicit layers",
    "text": "Implicit layers"
  },
  {
    "objectID": "05_dnn.html#feedforward-neural-networks",
    "href": "05_dnn.html#feedforward-neural-networks",
    "title": "Neural network: layers and architectures",
    "section": "Feedforward Neural Networks",
    "text": "Feedforward Neural Networks"
  },
  {
    "objectID": "05_dnn.html#convolutional-neural-networks",
    "href": "05_dnn.html#convolutional-neural-networks",
    "title": "Neural network: layers and architectures",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks"
  },
  {
    "objectID": "05_dnn.html#autoencoders",
    "href": "05_dnn.html#autoencoders",
    "title": "Neural network: layers and architectures",
    "section": "Autoencoders",
    "text": "Autoencoders"
  },
  {
    "objectID": "05_dnn.html#recurrent-neural-networks",
    "href": "05_dnn.html#recurrent-neural-networks",
    "title": "Neural network: layers and architectures",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks"
  },
  {
    "objectID": "05_dnn.html#transformers",
    "href": "05_dnn.html#transformers",
    "title": "Neural network: layers and architectures",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "05_dnn.html#graph-neural-networks",
    "href": "05_dnn.html#graph-neural-networks",
    "title": "Neural network: layers and architectures",
    "section": "Graph Neural Networks",
    "text": "Graph Neural Networks"
  },
  {
    "objectID": "05_openai.html",
    "href": "05_openai.html",
    "title": "Scientific Computation at large",
    "section": "",
    "text": "# Set your secret API key\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")"
  }
]