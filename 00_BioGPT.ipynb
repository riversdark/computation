{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OLRNFrPmslxJ"
      },
      "source": [
        "# BioGPT\n",
        "\n",
        "The BioGPT-Large model with 1.5B paramters hosted at [huggingface](https://huggingface.co/microsoft/biogpt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QIqWDusYqGzF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BioGptTokenizer, BioGptForCausalLM, set_seed, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WLoWYFpdrOg2"
      },
      "outputs": [],
      "source": [
        "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
        "model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beam-search decoding. The beam size determines the number of hypotheses to consider at each step. The higher the beam size, the more accurate the results, but the slower the inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'to differentiate germline and sporadic mutations, we need: (1) a better understanding of the mechanisms of germline mutation; (2) a better understanding of the mechanisms of somatic mutation; (3) a better understanding of the mechanisms of somatic mutation; (4) a better understanding of the mechanisms of germline mutation; (5) a better understanding of the mechanisms of somatic mutation; (6) a better understanding of the mechanisms of germline mutation; (7) a better understanding of the mechanisms of somatic mutation; and (8) a better understanding of the mechanisms of germline mutation.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "set_seed(43)\n",
        "sentence = \"to differentiate germline and sporadic mutations, we need:\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    beam_output = model.generate(**inputs,\n",
        "                                min_length=100,\n",
        "                                max_length=1024,\n",
        "                                num_beams=10,\n",
        "                                early_stopping=True\n",
        "                                )\n",
        "tokenizer.decode(beam_output[0], skip_special_tokens=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yHrPWFKnrTfm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'to differentiate germline and sporadic mutations, we need: (1) methods / assays not exclusively focused on germline ones such as the identification of new disease-associated single nucleotide variants and next-generation sequencing strategies able to characterize non-inherited disease-causing variants'}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "generator(sentence, max_length=50, num_return_sequences=1, do_sample=True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is how to use this model to get the features of a given text in PyTorch:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BioGptTokenizer, BioGptForCausalLM\n",
        "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
        "model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "jax",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "fb41119831cb00bc1e889f085d25b8530a38dc317bce3a16e7dee13931bb6e27"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
